# Дистилляция AR трансформера в masked diffusion LLM

## Обозначения

- $x_0 = (x_1,\dots,x_L)$: последовательность токенов из корпуса.
- $t \sim \mathcal{U}(0,1)$: коэффициент маскирования.
- $x_t$: последовательность, где каждый токен маскируется независимо с вероятностью $t$; M — токен маски.
- Студент: $p_\theta(\cdot \mid x_t)$ предсказывает все маскированные токены одновременно из видимого контекста.
- Учитель: AR трансформер, предсказывающий логиты следующего токена.
- Для SFT (промпт-ответ): разбиение $x_0 = [p_0; r_0]$ и маскирование только в $r_0$.

## 1) Базовый лосс MDM

**Pretrain:**

$$\mathcal{L}_{\text{MDM}} = \mathbb{E}_{x_0,\,t,\,x_t}\!\left[ - \frac{1}{L}\sum_{i=1}^L \mathbf{1}[x_i=\text{M}] \,\log p_\theta(x_i \mid x_t) \right]$$

**SFT (маскирование только ответа):**

$$\mathcal{L}_{\text{MDM-cond}} = \mathbb{E}_{(p_0,r_0),\,t,\,r_t}\!\left[ - \frac{1}{|r_0|}\sum_{i \in \mathrm{resp}} \mathbf{1}[r_i=\text{M}] \,\log p_\theta(r_i \mid p_0, r_t) \right]$$

## 2) Цель KD: KL дивергенция на soft таргеты учителя

Пусть $q_T(\cdot \mid \mathrm{context}_i)$ — распределение soft таргетов учителя для маскированной позиции $i$. Определим KD лосс с температурой $T$:

$$\mathcal{L}_{\text{KD}} = \mathbb{E}\!\left[ \frac{1}{L}\sum_{i=1}^L \mathbf{1}[x_i=\text{M}]\; T^2\,\mathrm{KL}\!\Big( q_T(\cdot \mid \mathrm{context}_i)\,\big\Vert\, p_\theta^T(\cdot \mid x_t) \Big) \right]$$

где $q_T = \mathrm{softmax}(z_T/T)$ и $p_\theta^T = \mathrm{softmax}(z_\theta/T)$; множитель $T^2$.

Объединённая функция потерь:

$$\mathcal{L} = (1-\lambda)\,\mathcal{L}_{\text{MDM или MDM-SFT}} + \lambda\,\mathcal{L}_{\text{KD}},\quad \lambda \in [0,1]$$

## 3) Как получить soft таргеты от учителя, который генерирует слева направо

Студенту нужны в $q_T(x_i \mid \text{left},\text{right})$, но учитель предсказывает $p_T(\cdot \mid \text{left})$. Для $i$-го маскированного токена:

$$p_T(x_i{=}v \mid \text{left}, \text{right}) \;\propto\; p_T(v \mid \text{left})\;\cdot\; p_T(\text{right} \mid \text{left}, v)$$

Переходим к логарифмам:

$$\begin{align}
\log p_T(\text{right} \mid \text{left}, v) &= \sum_{j=i+1}^{L} \log p_T\!\big(x_j \mid \text{left}, v, x_{i+1:j-1}\big) \\
s(v) &\triangleq \log p_T(v \mid \text{left}) + \log p_T(\text{right} \mid \text{left}, v) \\
q_T(v \mid \text{left}, \text{right}) &= \mathrm{softmax}_v\!\big(s(v)/T\big)
\end{align}$$

Можно ограничить кандидатов top-$K$ из $p_T(v \mid \text{left})$ для эффективности.

### Вывод и нормализация

По определению условной вероятности и цепному правилу:

$$q_T(v \mid \text{left}, \text{right}) \;=\; \frac{p_T(\text{left}, v, \text{right})}{\sum_{u\in\mathcal{V}} p_T(\text{left}, u, \text{right})} \;=\; \frac{p_T(v\mid\text{left})\,p_T(\text{right}\mid\text{left}, v)}{\sum_{u\in\mathcal{V}} p_T(u\mid\text{left})\,p_T(\text{right}\mid\text{left}, u)}$$

Нормирующая константа:

$$Z \;=\; \sum_{u\in\mathcal{V}} p_T(u\mid\text{left})\,p_T(\text{right}\mid\text{left}, u)$$

### Температура

Применяем температуру $T > 0$ при нормализации: $q_T(\cdot) = \mathrm{softmax}(s(\cdot)/T)$. Большие значения $T$ дают более мягкие цели.

## 4) Приближение Монте-Карло для дистилляции между архитектурами

При дистилляции автарегрессивной модели-учителя в маскированно-диффузионную модель-студента необходимо передать знания учителя о распределениях токенов в двунаправленный предиктор.

Студент должен моделировать для каждого маскированного индекса $i$ условное распределение

$$p_\theta(x_i \mid x_t)$$

то есть распределение истинного токена в позиции $i$ при видимом (немаскированном) контексте в $x_t$.

### Маргинальное распределение учителя над маскированными токенами

AR учитель определяет совместное распределение над полными последовательностями:

$$p_T(x_0) = \prod_{j=1}^{L} p_T(x_j \mid x_{<j})$$

где $x_{<j} = (x_1, \ldots, x_{j-1})$. Студент учится соответствовать условному распределению учителя $q_T(x_i \mid x_t)$, которое может быть выражено:

$$q_T(x_i \mid x_t) = \sum_{z \in \mathcal{V}^{\text{masked}}} p_T(x_i \mid x_t, z)\; P_T(z \mid x_t)$$

где $z$ представляет совокупность всех маскированных токенов кроме $x_i$, а $P_T(z \mid x_t)$ — апостериорное распределение учителя этих токенов при видимом контексте.

### Приближение Монте-Карло

Для аппроксимации маргинального распределения используем Монте-Карло выборку. Пусть $\{z^{(k)}\}_{k=1}^K$ — $K$ независимых выборок маскированных токенов, полученных путём AR выборки из учителя при фиксированных видимых токенах $x_t$:

$$q_T(x_i \mid x_t) = \mathbb{E}_{z \sim P_T(z \mid x_t)} [\, p_T(x_i \mid x_t, z) \,] \;\approx\; \frac{1}{K} \sum_{k=1}^{K} p_T(x_i \mid x_t, z^{(k)})$$

Каждый член $p_T(x_i \mid x_t, z^{(k)})$ вычисляется путём запуска учителя авторегрессивно на завершённой последовательности $x^{(k)} = \text{fill}(x_t, z^{(k)})$, где маскированные позиции заменены отобранными токенами.

При $K \to \infty$ оценка Монте-Карло сходится к истинному маргинальному распределению.

### Функция потерь для дистилляции

Маргинальное распределение учителя, аппроксимированное Монте-Карло $\widehat{q}_T(x_i \mid x_t)$, служит мягкой целью для студента. Мягкая потеря дистилляции определяется как кросс-энтропия между предсказанным распределением студента и этой целью:

$$\mathcal{L}_{\mathrm{KD}}(\theta) = -\, \mathbb{E}_{x_t} \left[ \frac{1}{L} \sum_{i:\,x_{t,i} = [\text{MASK}]} \sum_{v \in \mathcal{V}} \widehat{q}_T(v \mid x_t) \log p_\theta(v \mid x_t) \right]$$

где

$$\widehat{q}_T(v \mid x_t) = \frac{1}{K}\sum_{k=1}^{K} p_T(v \mid x_t, z^{(k)})$$

Это эквивалентно минимизации дивергенции Кульбака–Лейблера $\mathrm{KL}(\widehat{q}_T(\cdot \mid x_t) \,\|\, p_\theta(\cdot \mid x_t))$.

Также логиты учителя можно смягчить с помощью параметра температуры $T > 1$:

$$p_T(v \mid \cdot) = \text{softmax}\left(\frac{\text{logits}_T(v)}{T}\right)$$

## 5) Fill in the middle

Многие кодовые LLM (в том числе QwenCoder) умеют предсказывать токен в середине. В таком случае можно просто посчитать KL дивергенцию между логитами учителя и ученика для этого токена.
